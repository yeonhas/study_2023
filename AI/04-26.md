# 완전 연결층 perceptron

입력data의 종류 :1D, 2D, 시계열 data(time series) 등

완전 연결층 : 입력의 모든 노드가 출력의 모든 노드와 연결된 층

perceptron, dense layer


# 활성화 함수 activation function

출력의 발산(diversion)을 막기 위해 0~1의 값으로 재조정해야 함

ReLU가 가장 많이 쓰임(양수는 그대로 음수는 0으로)

SoftMax는 분류 모델에서 제일 마지막에 쓰임(출력 값들의 합이 1이 되도록 쓰임)

# 다층 신경망 multi-layer perceptron

완전연결층을 여러 개 쌓아서 네트워크를 만듦

미지수 = layer의 수 x edge의 수

n개의 미지수를 가진 방정식을 풀기 위해서는 n의 값이 있으면 되지만, 다양한 noise가 있으므로 훨씬 더 많은 값이 필요
